# streamlit_app.py
import hashlib
import streamlit as st
import time
from pathlib import Path
import io
import requests
import threading
from typing import List, Dict
import json
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import re
import itertools
from openai import OpenAI
import os
import base64
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import openpyxl
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from matplotlib import rc
import matplotlib.font_manager as fm


# API í‚¤ ê¸°ë³¸ê°’ ì„¤ì •
llm_api_key = st.secrets["llm_api_key"]
naver_client_id = st.secrets["naver_client_id"]
naver_client_secret = st.secrets["naver_client_secret"]
xi_api_key = st.secrets["xi_api_key"]
voice_id = st.secrets["voice_id"]


st.set_page_config(
    initial_sidebar_state="collapsed",
    
)

# ì»¤ìŠ¤í…€ CSS ì¶”ê°€
st.markdown("""
    <style>
    @font-face {
        font-family: 'MaruBuBareun_hipiriBold';
         src: url('https://fastly.jsdelivr.net/gh/projectnoonnu/naverfont_01@1.0/Bareun_hipi.woff') format('woff');
        font-weight: bold;
        font-style: normal;
    }
    .custom-title {
        font-family: 'MaruBuBareun_hipiriBold', sans-serif;
        font-size: 3em; /* ì›í•˜ëŠ” í¬ê¸°ë¡œ ì¡°ì • */
        font-weight: bold;
    }
    .custom-title1 {
        font-family: 'MaruBuBareun_hipiriBold', sans-serif;
        font-size: 16px; /* ì›í•˜ëŠ” í¬ê¸°ë¡œ ì¡°ì • */
        font-weight: bold
        font-style: normal;
    }
    fixed-title {
        position: fixed;
        top: 5;
        width: 100%;
        z-index: 9999;
        padding: 8px;
    }
    </style>
    """, unsafe_allow_html=True)

# í˜ì´ì§€ ì œëª©
st.markdown('<h1 class="custom-title"> ì‹ í•œì¹´ë“œ ì‹ ì…ì‚¬ì› - CEO ì»¤ë®¤ë‹ˆì¼€ì´ì…˜  </h1>', unsafe_allow_html=True)
st.markdown('<h3 class="custom-title1"> ì‹ ì…ì‚¬ì›ë“¤ì€ ê¶ê¸ˆí•œ ì‚¬í•­ì„ ììœ ë¡­ê²Œ ë¬¼ì–´ë³´ì„¸ìš” ğŸ™‹â€â™€ï¸ğŸ™‹â€â™‚ï¸ </h3>', unsafe_allow_html=True)
    
class StreamlitNewsSearchSystem:
    def __init__(self, naver_client_id: str, naver_client_secret: str, llm_api_key: str, xi_api_key: str, voice_id: str):
        self.naver_client_id = naver_client_id
        self.naver_client_secret = naver_client_secret
        self.llm_api_key = llm_api_key
        self.xi_api_key = xi_api_key  # ElevenLabs API í‚¤
        self.voice_id = voice_id  # ElevenLabs Voice ID
        self.client = OpenAI(api_key=llm_api_key)
    
    def extract_keywords(self, query: str, progress_bar) -> List[str]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            progress_bar.progress(10)
            st.write("ì •ë³´ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”")
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ì‹ í•œì¹´ë“œ ë‹µë³€ ê´€ë ¨ ë´‡ì…ë‹ˆë‹¤. íšŒì‚¬ ê´€ë ¨ ë¬¸ì˜ë‚˜ ì§ˆë¬¸ì´ ë“¤ì–´ì™”ì„ ë•Œ, ì…ë ¥ëœ ì§ˆë¬¸ì—ì„œ í•µì‹¬ ê²€ìƒ‰ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í˜•íƒœë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": query
                    }
                ]
            )
            
            keywords = response.choices[0].message.content.split(',')
            keywords = [keyword.strip() for keyword in keywords]
            
            if 'ë¬¸ë™ê¶Œ' not in keywords:
                keywords.insert(0, 'ë¬¸ë™ê¶Œ')
            
            progress_bar.progress(20)
            return keywords
            
        except Exception as e:
            st.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise e

    def clean_html_text(self, html_content: str) -> str:
        """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\.,!?"\'-]', '', text)
        return text.strip()

    def get_full_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë‚´ìš© í¬ë¡¤ë§"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            if 'news.naver.com' in url:
                response = requests.get(url, headers=headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                for selector in ['#articleBody', '#articleBodyContents', '#newsEndContents']:
                    article_content = soup.select_one(selector)
                    if article_content:
                        break
                        
            elif 'ceoscoredaily.com' in url:
                response = requests.get(url, headers=headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                article_content = soup.select_one('.view_cont')
                
            if article_content:
                for unnecessary in article_content.select('script, style, header, footer'):
                    unnecessary.decompose()
                return self.clean_html_text(str(article_content))
                    
            return "ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            return "ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def search_naver_news(self, keywords: List[str], progress_bar, display: int = 5) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            query = ' '.join(keywords)
            url = "https://openapi.naver.com/v1/search/news.json"
            headers = {
                "X-Naver-Client-Id": self.naver_client_id,
                "X-Naver-Client-Secret": self.naver_client_secret
            }
            params = {
                "query": query,
                "display": display,
                "sort": "date"
            }
            
            progress_bar.progress(30)
            
            response = requests.get(url, headers=headers, params=params)
            response.raise_for_status()
            news_items = response.json()['items']
            
            filtered_items = []
            with st.spinner("ê´€ë ¨ ê¸°ì‚¬ ë¶„ì„ ì¤‘..."):
                for item in news_items:
                    title = re.sub('<[^<]+?>', '', item['title'])
                    
                    shinhan_keywords = ['ì‹ í•œì¹´ë“œ', 'ë¬¸ë™ê¶Œ']
                    has_shinhan_keyword = any(keyword in title for keyword in shinhan_keywords)
                    
                    pub_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900')
                    if pub_date.year >= 2023 and has_shinhan_keyword:
                        full_content = self.get_full_article_content(item['link'])
                        item['full_content'] = full_content
                        filtered_items.append(item)
            
            progress_bar.progress(40)
            return filtered_items
            
        except Exception as e:
            return []

    def search_with_progressive_keywords(self, keywords: List[str], progress_bar, display: int = 5) -> List[Dict]:
        """í‚¤ì›Œë“œë¥¼ ì ì§„ì ìœ¼ë¡œ ì¤„ì—¬ê°€ë©° ê²€ìƒ‰"""
        try:
            all_combinations = []
            other_keywords = [k for k in keywords if k != 'ë¬¸ë™ê¶Œ']
            
            # ê²€ìƒ‰ ì‹œì‘ ì•Œë¦¼
            with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘..."):
                for i in range(len(other_keywords), 0, -1):
                    for combo in itertools.combinations(other_keywords, i):
                        keywords_combo = ['ë¬¸ë™ê¶Œ'] + list(combo)
                        all_combinations.append(keywords_combo)
                    
                    for combo in all_combinations:
                        try:
                            news_items = self.search_naver_news(combo, progress_bar, display)
                            if news_items:
                                return news_items
                        except Exception as e:
                            continue
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ
                st.info("ê´€ë ¨ëœ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")
                return []
                
        except Exception as e:
            st.error("ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            return []

    def analyze_news_content(self, news_items: List[Dict], original_query: str, progress_bar) -> str:
        """ë‰´ìŠ¤ ë‚´ìš© ë¶„ì„ ê°œì„  ë²„ì „"""
        try:
            st.write("ë‰´ìŠ¤ ë‚´ìš© ë¶„ì„ ì¤‘...")
            progress_bar.progress(60)
            
            # ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ì •ë¦¬
            news_metadata = []
            news_contents = []
            
            for item in news_items:
                # ë©”íƒ€ë°ì´í„°ì™€ ì½˜í…ì¸  ë¶„ë¦¬
                metadata = {
                    "title": item['title'],
                    "date": item['pubDate'],
                    "url": item['link']
                }
                news_metadata.append(metadata)
                
                # ì „ì²´ ì½˜í…ì¸ ëŠ” ë³„ë„ë¡œ ì €ì¥
                news_contents.append(item['full_content'])
            
            # í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê°•í™”
            system_prompt = """ë‹¹ì‹ ì€ ì‹ í•œì¹´ë“œì˜ CEOì™€ ì‹ ì…ì‚¬ì›ì˜ ì†Œí†µì„ ë•ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:
            1. ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
            2. ë¬¸ë™ê¶Œ ì‚¬ì¥ë‹˜ì˜ ì‹¤ì œ ë°œì–¸ì´ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì¸ìš©í•©ë‹ˆë‹¤.
            3. ì‹¤ì œ ë°œì–¸ì´ ì—†ë‹¤ë©´ ê¸°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¼ê´€ëœ ë©”ì‹œì§€ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
            4. ì‹ í•œì¹´ë“œì˜ ì „ëµê³¼ ë°©í–¥ì„±ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤."""

            user_prompt = f"""ì›ë³¸ ì§ˆë¬¸: {original_query}

    ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©:
    {json.dumps(news_contents, ensure_ascii=False, indent=2)}

    ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°:
    {json.dumps(news_metadata, ensure_ascii=False, indent=2)}

    ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

    [ë¬¸ë™ê¶Œ ì‚¬ì¥ë‹˜ ë§ì”€]
    (ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ 30ì´ˆ ë¶„ëŸ‰ì˜ ë‹µë³€)

    [ì°¸ê³  ê¸°ì‚¬]
    - ì œëª©: (ê´€ë ¨ì„± ë†’ì€ ìˆœì„œëŒ€ë¡œ)
    - ë‚ ì§œ: (ê¸°ì‚¬ ë‚ ì§œ)
    - URL: (ê¸°ì‚¬ ë§í¬)
    - ê´€ë ¨ ë‚´ìš©: (ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©)

    [ì‹ ì…ì‚¬ì› ê°€ì´ë“œ]
    (ì•ì„  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹ ì…ì‚¬ì›ë“¤ì´ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ê°€ì´ë“œë¼ì¸)"""

            progress_bar.progress(70)
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.2
            )
            
            progress_bar.progress(80)
            return response.choices[0].message.content
            
        except Exception as e:
            st.error(f"ë‰´ìŠ¤ ë‚´ìš© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise e

    def extract_tts_content(self, text: str) -> str:
        """TTSìš© ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            if '[ì°¸ê³  ê¸°ì‚¬]' in text:
                text = text.split('[ì°¸ê³  ê¸°ì‚¬]')[0].strip()
            st.markdown(f"#### TTSìš© í…ìŠ¤íŠ¸:\n{text}")  # í…ìŠ¤íŠ¸ë¥¼ UIì— ì¶œë ¥
            return text
            
        except Exception as e:
            st.error(f"TTS ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return text
    @st.cache_data
    def generate_tts_with_elevenlabs(_self, text: str, xi_api_key: str, voice_id: str) -> str:
        """ElevenLabs APIë¥¼ í˜¸ì¶œí•˜ì—¬ TTS ìƒì„±"""
        try:
            # í…ìŠ¤íŠ¸ì˜ ê³ ìœ  í•´ì‹œê°’ ìƒì„± (ìºì‹± í‚¤ë¡œ í™œìš©)
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
            audio_file = f"output_{text_hash}.mp3"
            
            # ìºì‹œëœ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ í•´ë‹¹ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
            if Path(audio_file).exists():
                return audio_file
            
            url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
            headers = {
                "Accept": "audio/mpeg",
                "Content-Type": "application/json",
                "xi-api-key": xi_api_key
            }
            data = {
                "text": text,
                "model_id": "eleven_multilingual_v2",
                "voice_settings": {
                    "stability": 0.75,
                    "similarity_boost": 0.9,
                    "style": 0.2,
                    "use_speaker_boost": True,
                    "speaking_rate": 1.2
                     
                }
            }

            response = requests.post(url, json=data, headers=headers, stream=True)
            if response.status_code == 200:
                audio_file = "output.mp3"
                with open(audio_file, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=1024):
                        if chunk:
                            f.write(chunk)
                return audio_file
            else:
                st.error(f"ElevenLabs API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}, {response.text}")
                return None

        except Exception as e:
            st.error(f"ElevenLabs TTS ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    
    def speak_result(self, text: str) -> None:
        """Streamlitì„ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ ìŒì„± ì¶œë ¥"""
        try:
            # [ë¬¸ë™ê¶Œ ì‚¬ì¥ë‹˜ ë§ì”€] ì„¹ì…˜ë§Œ ì¶”ì¶œ
            if "[ë¬¸ë™ê¶Œ ì‚¬ì¥ë‹˜ ë§ì”€]" in text:
                tts_content = text.split("[ë¬¸ë™ê¶Œ ì‚¬ì¥ë‹˜ ë§ì”€]")[1].split("[ì°¸ê³  ê¸°ì‚¬]")[0].strip()
            else:
                return

            # TTS ìƒì„± í˜¸ì¶œ
            audio_path = self.generate_tts_with_elevenlabs(tts_content, self.xi_api_key, self.voice_id)

            if audio_path and Path(audio_path).exists():
                # Streamlitì„ í™œìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ì¬ìƒ
                with open(audio_path, "rb") as audio_file:
                    audio_bytes = audio_file.read()
                    st.audio(audio_bytes, format="audio/mp3")
            else:
                st.error("ìŒì„± íŒŒì¼ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"TTS ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def save_message(message, role):
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    st.session_state["messages"].append({"message": message, "role": role})

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ assets í´ë” ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ASSETS_DIR = os.path.join(SCRIPT_DIR, 'static')

def get_avatar_path(role: str) -> str:
    """ì´ë¯¸ì§€ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ë°˜í™˜"""
    image_path = os.path.join(ASSETS_DIR, f'{role}_character.png')
    if os.path.exists(image_path):
        return image_path
    print(f"Warning: Image not found at {image_path}")  # ë””ë²„ê¹…ìš©
    return None

def send_message(message, role, save=True):
    """ë©”ì‹œì§€ë¥¼ ì±„íŒ…ì°½ì— í‘œì‹œ"""
    avatar_path = get_avatar_path('human' if role == 'human' else 'bot')
    try:
        with st.chat_message(role, avatar=avatar_path):
            st.markdown(message, unsafe_allow_html=True)
    except Exception as e:
        print(f"Error displaying message with avatar: {e}")
        with st.chat_message(role):
            st.markdown(message, unsafe_allow_html=True)
    if save:
        save_message(message, role)

def get_image_as_base64(image_path):
    """ì´ë¯¸ì§€ë¥¼ Base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
    try:
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return ""

# ë°°ê²½ ì´ë¯¸ì§€ ì¶”ê°€
bg_image_path = "static/bg.png"  # ë°°ê²½ ì´ë¯¸ì§€ ê²½ë¡œ
if Path(bg_image_path).exists():
    bg_image_base64 = get_image_as_base64(bg_image_path)
    st.markdown(
        f"""
        <style>
        /* ì „ì²´ í˜ì´ì§€ ë°°ê²½ */
        html {{
            background-image: url("data:image/png;base64,{bg_image_base64}");
            background-size: cover;
            background-attachment: fixed;
            background-repeat: no-repeat;
        }}

        /* í…ìŠ¤íŠ¸ ì…ë ¥ì°½ í•˜ë‹¨ ì˜ì—­ (stChatInput) */
        [data-testid="stApp"]{{
            background-image: url("data:image/png;base64,{bg_image_base64}");
            background-size: cover;
    /       background: rgba(255, 255, 255, 0); /* íˆ¬ëª…í™” *
        }}
        
        /* ì‚¬ì´ë“œë°” ë°°ê²½ íˆ¬ëª…í™” */
        [data-testid="stSidebar"] {{
            background: rgba(255, 255, 255, 0); /* íˆ¬ëª…í™” */
        }}
        
        /* ì‚¬ì´ë“œë°” ë°°ê²½ íˆ¬ëª…í™” */
        [data-testid="stHeader"] {{
            background: rgba(255, 255, 255, 0); /* íˆ¬ëª…í™” */
        }}
        
        /* ì‚¬ì´ë“œë°” ë°°ê²½ íˆ¬ëª…í™” */
        [data-testid="stBottom"] {{
            background: rgba(255, 255, 255, 0); /* íˆ¬ëª…í™” */
        }}
        
        /* ì‚¬ì´ë“œë°” ë°°ê²½ íˆ¬ëª…í™” */
        [data-testid="stBottom"] > div {{
            background: rgba(255, 255, 255, 0); /* íˆ¬ëª…í™” */
        }}
        </style>
        """,
        unsafe_allow_html=True
    )
else:
    st.warning("ë°°ê²½ ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

def analyze_uploaded_file(file):
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        if file.name.endswith(".csv"):
            df = pd.read_csv(file)  # CSV íŒŒì¼ ì½ê¸°
        elif file.name.endswith(".xlsx"):
            df = pd.read_excel(file)  # XLSX íŒŒì¼ ì½ê¸°
        else:
            st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë˜ëŠ” XLSX íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return None, None  # ì˜ëª»ëœ íŒŒì¼ í˜•ì‹ ì²˜ë¦¬

        # íŒŒì¼ ì—´ ì´ë¦„ í™•ì¸ ë° ë°ì´í„° í‘œì‹œ
        st.write("### ì—…ë¡œë“œëœ íŒŒì¼ì˜ ì—´ ëª©ë¡:", list(df.columns))

        # ì—…ë¡œë“œëœ ë°ì´í„°ì˜ ì¼ë¶€ ë³´ì—¬ì£¼ê¸°
        st.markdown("#### íŒŒì¼ ë‚´ìš© (ìƒìœ„ 5ì¤„)")
        st.dataframe(df.head())  # ë°ì´í„°í”„ë ˆì„ì˜ ìƒìœ„ 5ì¤„ í‘œì‹œ

        if "text" not in df.columns:
            # 'text' ì—´ì´ ì—†ìœ¼ë©´ ì‚¬ìš©ìì—ê²Œ ì—´ ì„ íƒ ì˜µì…˜ ì œê³µ
            text_column = st.selectbox(
                "í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í¬í•¨í•œ ì—´ì„ ì„ íƒí•˜ì„¸ìš”:", df.columns
            )
        else:
            text_column = "text"  # 'text' ì—´ì´ ìˆìœ¼ë©´ ìë™ ì„ íƒ

        # ì„ íƒëœ ì—´ì˜ í…ìŠ¤íŠ¸ ë°ì´í„° ê²°í•©
        text_data = " ".join(df[text_column].dropna().astype(str))
        return text_data, df
    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None, None  # ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ



def generate_wordcloud_from_keywords(keyword_data):
    """í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (í•œê¸€ ì§€ì›)"""
    try:
        # í‚¤ì›Œë“œ ë°ì´í„° í™•ì¸
        if not keyword_data or not isinstance(keyword_data, list):
            st.error("ìœ íš¨í•œ í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì›Œë“œ í´ë¼ìš°ë“œ ì…ë ¥ ë°ì´í„° ìƒì„±
        wordcloud_input = {item["keyword"]: item["count"] for item in keyword_data if "keyword" in item and "count" in item}

        if not wordcloud_input:
            st.error("í‚¤ì›Œë“œ ë°ì´í„°ì— ë¹ˆë„ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        # ë””ë²„ê¹…: ì…ë ¥ê°’ í™•ì¸
        st.write("ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„°:", wordcloud_input)

        # í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œ í°íŠ¸ ìë™ íƒìƒ‰)
        font_path = None
        for font in fm.findSystemFonts(fontpaths=None, fontext="ttf"):
            if "NanumGothic" in font or "Malgun" in font:  # í•œê¸€ ì§€ì› í°íŠ¸ ì°¾ê¸°
                font_path = font
                break

        if not font_path:
            st.error("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì— í•œê¸€ í°íŠ¸ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
            return

        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        wordcloud = WordCloud(font_path=font_path, width=800, height=400, background_color="white").generate_from_frequencies(wordcloud_input)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        st.pyplot(plt)

    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")



def analyze_text(text_data, response_data=None):
    """í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì°¨íŠ¸ë¡œ ì‹œê°í™”"""
    # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ (í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œë¶€í„° ìƒì„±)
    if text_data:
        word_list = text_data.split()
        word_freq = pd.Series(word_list).value_counts().head(10)
        freq_df = pd.DataFrame({"keyword": word_freq.index, "count": word_freq.values})
    elif response_data and "keyword_frequency" in response_data:
        freq_df = pd.DataFrame(response_data["keyword_frequency"])
    else:
        st.error("í…ìŠ¤íŠ¸ ë°ì´í„° ë˜ëŠ” í‚¤ì›Œë“œ ë¹ˆë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê°ì„± ë¶„ì„ ë°ì´í„°
    if response_data and "sentiment_analysis" in response_data:
        sentiment_data = response_data["sentiment_analysis"]
    else:
        sentiment_data = {"positive_score": 70, "negative_score": 20, "neutral_score": 10}

    total_score = sum(sentiment_data.values())
    normalized_score = (sentiment_data["positive_score"] / total_score) * 100

    # ì£¼ì œ ë¶„í¬ ë°ì´í„°
    if response_data and "topic_distribution" in response_data:
        topic_distribution = pd.DataFrame(response_data["topic_distribution"])
    else:
        topic_distribution = pd.DataFrame({
            "topic": ["ì£¼ì œ1", "ì£¼ì œ2", "ì£¼ì œ3"],
            "percentage": [40, 30, 30]
        })

    # ê²°ê³¼ ì‹œê°í™”
    st.markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼")
    
    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    st.markdown("#### ì£¼ìš” í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
    generate_wordcloud_from_keywords(freq_df.to_dict("records"))

    # í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ ì°¨íŠ¸
    st.markdown("#### ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„")
    fig_freq = px.bar(freq_df, x="keyword", y="count", title="ì£¼ìš” í‚¤ì›Œë“œ Top 10")
    st.plotly_chart(fig_freq, use_container_width=True)
    
    # ê°ì„± ë¶„ì„ ê²Œì´ì§€
    st.markdown("#### ê°ì„± ë¶„ì„")
    fig_sentiment = go.Figure(go.Indicator(
        mode="gauge+number",
        value=normalized_score,
        title={'text': "ê¸ì •ë„ ì§€ìˆ˜"},
        gauge={
            'axis': {'range': [0, 100]},
            'steps': [
                {'range': [0, 50], 'color': "lightgray"},
                {'range': [50, 100], 'color': "lightblue"}
            ]
        }
    ))
    st.plotly_chart(fig_sentiment, use_container_width=True)
    
    # ì£¼ì œ ë¶„í¬ íŒŒì´ ì°¨íŠ¸
    st.markdown("#### ì£¼ì œ ë¶„í¬")
    fig_topic = px.pie(topic_distribution, values="percentage", names="topic", title="ì£¼ì œ ë¶„í¬")
    st.plotly_chart(fig_topic, use_container_width=True)


        
# ì‚¬ì´ë“œë°”ì—ì„œ í…ìŠ¤íŠ¸ ë¶„ì„ ì±—ë´‡ ëª¨ë“œ ì¶”ê°€
def create_sidebar_with_text_analysis():
    """ì‚¬ì´ë“œë°”ì—ì„œ í…ìŠ¤íŠ¸ ë¶„ì„ ì±—ë´‡ ëª¨ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    with st.sidebar:
        st.markdown("### ğŸ¤– ì±—ë´‡ ëª¨ë“œ ì„ íƒ")
        
        # ëª¨ë“œ ì„ íƒ
        mode = st.radio(
            "ì›í•˜ì‹œëŠ” ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
            ["ê¸°ë³¸ ì±—ë´‡", "í…ìŠ¤íŠ¸ ë¶„ì„ ì±—ë´‡"],
            index=0,
            key="chat_mode"
        )
        
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.analysis_mode = (mode == "í…ìŠ¤íŠ¸ ë¶„ì„ ì±—ë´‡")

def format_analysis_results(analysis_results):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ëŒ€í™” ì´ë ¥ì— ì €ì¥í•˜ê¸° ìœ„í•œ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    formatted_result = "### ğŸ“Š í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼\n\n"
    
    # í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ í¬ë§·íŒ…
    if 'keyword_frequency' in analysis_results:
        formatted_result += "#### ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„\n"
        for keyword in analysis_results['keyword_frequency']:
            formatted_result += f"- {keyword['keyword']}: {keyword['count']}íšŒ\n"
        formatted_result += "\n"
    
    # ì£¼ì œ ë¶„í¬ í¬ë§·íŒ…
    if 'topic_distribution' in analysis_results:
        formatted_result += "#### ì£¼ì œ ë¶„í¬\n"
        for topic in analysis_results['topic_distribution']:
            formatted_result += f"- {topic['topic']}: {topic['percentage']}%\n"
        formatted_result += "\n"
    
    # ê°ì„± ë¶„ì„ í¬ë§·íŒ…
    if 'sentiment_analysis' in analysis_results:
        sentiment = analysis_results['sentiment_analysis']
        total_score = sum(sentiment.values())
        formatted_result += "#### ê°ì„± ë¶„ì„\n"
        formatted_result += f"- ê¸ì •: {(sentiment['positive_score']/total_score)*100:.1f}%\n"
        formatted_result += f"- ë¶€ì •: {(sentiment['negative_score']/total_score)*100:.1f}%\n"
        formatted_result += f"- ì¤‘ë¦½: {(sentiment['neutral_score']/total_score)*100:.1f}%\n\n"
    
    # ì£¼ìš” ì¸ì‚¬ì´íŠ¸ í¬ë§·íŒ…
    if 'key_insights' in analysis_results:
        formatted_result += "#### ì£¼ìš” ì¸ì‚¬ì´íŠ¸\n"
        for insight in analysis_results['key_insights']:
            formatted_result += f"- {insight}\n"
    
    return formatted_result

def save_analysis_to_history(st_state, analysis_results, uploaded_filename):
    """ë¶„ì„ ê²°ê³¼ì™€ ì°¨íŠ¸ë¥¼ ëŒ€í™” ì´ë ¥ì— ì €ì¥"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # ë¶„ì„ ê²°ê³¼ ë°ì´í„° ì €ì¥
    analysis_entry = {
        'timestamp': timestamp,
        'filename': uploaded_filename,
        'results': analysis_results,
        'type': 'analysis'  # ë©”ì‹œì§€ íƒ€ì…ì„ ë¶„ì„ìœ¼ë¡œ í‘œì‹œ
    }
    
    # ëŒ€í™” ì´ë ¥ì— ì €ì¥
    if 'messages' not in st_state:
        st_state.messages = []
    st_state.messages.append({
        "message": analysis_entry,
        "role": "ai",
        "type": "analysis"  # ë¶„ì„ íƒ€ì… ë©”ì‹œì§€ì„ì„ í‘œì‹œ
    })
    
    # ê²€ìƒ‰ ì´ë ¥ì—ë„ ì €ì¥
    if 'search_history' not in st_state:
        st_state.search_history = []
    st_state.search_history.append(analysis_entry)

def display_analysis_results(analysis_results, filename=""):
    """ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¨íŠ¸ì™€ í•¨ê»˜ í‘œì‹œ"""
    st.markdown(f"### ğŸ“Š íŒŒì¼ ë¶„ì„: {filename}")
    
    # í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ ì°¨íŠ¸
    if 'keyword_frequency' in analysis_results:
        st.markdown("#### ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„")
        keyword_df = pd.DataFrame(analysis_results['keyword_frequency'])
        keyword_chart = px.bar(keyword_df, x="keyword", y="count", 
                             title="í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜",
                             labels={"count": "ì¶œí˜„ íšŸìˆ˜", "keyword": "í‚¤ì›Œë“œ"})
        st.plotly_chart(keyword_chart, use_container_width=True)
    
    # ì£¼ì œ ë¶„í¬ íŒŒì´ ì°¨íŠ¸
    if 'topic_distribution' in analysis_results:
        st.markdown("#### ì£¼ì œ ë¶„í¬")
        topic_df = pd.DataFrame(analysis_results['topic_distribution'])
        topic_chart = px.pie(topic_df, values="percentage", names="topic", 
                           title="ì£¼ì œë³„ ë¶„í¬")
        st.plotly_chart(topic_chart, use_container_width=True)
    
    # ê°ì„± ë¶„ì„ ê²Œì´ì§€
    if 'sentiment_analysis' in analysis_results:
        st.markdown("#### ê°ì„± ë¶„ì„")
        sentiment = analysis_results['sentiment_analysis']
        total_score = sum(sentiment.values())
        positive_ratio = (sentiment["positive_score"] / total_score) * 100
        
        sentiment_chart = go.Figure(go.Indicator(
            mode="gauge+number",
            value=positive_ratio,
            title={'text': "ê¸ì •ë„ ë¹„ìœ¨"},
            gauge={
                'axis': {'range': [0, 100]},
                'steps': [
                    {'range': [0, 50], 'color': "lightgray"},
                    {'range': [50, 100], 'color': "lightblue"}
                ]
            }
        ))
        st.plotly_chart(sentiment_chart, use_container_width=True)
    
    # ì£¼ìš” ì¸ì‚¬ì´íŠ¸
    if 'key_insights' in analysis_results:
        st.markdown("#### ì£¼ìš” ì¸ì‚¬ì´íŠ¸")
        for insight in analysis_results['key_insights']:
            st.markdown(f"- {insight}")


def main(query):
     # ëª¨ë“œì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬
        if st.session_state.analysis_mode:
            # í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“œì¼ ë•Œ
            if "data" not in st.session_state or st.session_state.data is None:
                st.error("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                return
            
            # LLMì—ê²Œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ë¶„ì„ ìˆ˜í–‰
            analysis_prompt = f"""
            ì•„ë˜ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì •í™•íˆ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

            ë°ì´í„°:
            {text_data[:1000]}  # í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ë§Œ ì „ë‹¬ (ê¸¸ì´ ì œí•œ ëŒ€ë¹„)

            ë¶„ì„ ìš”êµ¬ì‚¬í•­:
            1. í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ ë¶„ì„: "keyword_frequency"ì— JSON ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”. í˜•ì‹ì€ {{"keyword": "ë‹¨ì–´", "count": ìˆ«ì}}ì…ë‹ˆë‹¤.
            2. ì£¼ì œ ë¶„í¬: "topic_distribution"ì— JSON ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”. í˜•ì‹ì€ {{"topic": "ì£¼ì œ", "percentage": ìˆ«ì}}ì…ë‹ˆë‹¤.
            3. ê°ì„± ë¶„ì„: "sentiment_analysis"ì— JSON ê°ì²´ë¡œ ê¸ì •, ë¶€ì •, ì¤‘ë¦½ ë¹„ìœ¨ì„ ë°˜í™˜í•˜ì„¸ìš”. í˜•ì‹ì€ {{"positive_score": ìˆ«ì, "negative_score": ìˆ«ì, "neutral_score": ìˆ«ì}}ì…ë‹ˆë‹¤.
            4. ì£¼ìš” ì¸ì‚¬ì´íŠ¸: "key_insights"ì— JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”. í˜•ì‹ì€ ["ì¸ì‚¬ì´íŠ¸1", "ì¸ì‚¬ì´íŠ¸2", ...]ì…ë‹ˆë‹¤.

            JSON í˜•ì‹ ì˜ˆ:
            {{
                "keyword_frequency": [
                    {{"keyword": "example", "count": 10}},
                    ...
                ],
                "topic_distribution": [
                    {{"topic": "example_topic", "percentage": 50}},
                    ...
                ],
                "sentiment_analysis": {{
                    "positive_score": 60,
                    "negative_score": 30,
                    "neutral_score": 10
                }},
                "key_insights": [
                    "Insight 1",
                    "Insight 2"
                ]
            }}
            """
            
            try:
                analysis_response = st.session_state.search_system.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": analysis_prompt}
                    ],
                    temperature=0.2
                )
                
                raw_response = response.choices[0].message.content.strip()

                # ì „ì²˜ë¦¬: ì‘ë‹µì—ì„œ JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
                if raw_response.startswith("```json"):
                    raw_response = raw_response.strip("```json").strip("```")

                # JSON íŒŒì‹±
                analysis_results = json.loads(raw_response)
                st.write("íŒŒì‹± ì„±ê³µ:", analysis_results)

            except json.JSONDecodeError as e:
                st.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                st.write("LLM ì‘ë‹µ ë‚´ìš©:", raw_response)
            except Exception as e:
                st.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                
                # ì‘ë‹µ í…ìŠ¤íŠ¸ í‘œì‹œ
                st.markdown("### ğŸ“ ë¶„ì„ ê²°ê³¼")
                st.markdown(analysis_results.get("response", "ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
                
                # ì‹œê°í™” ë°ì´í„° í‘œì‹œ
                visualizations = analysis_results.get("visualizations", [])
                for viz in visualizations:
                    chart_type = viz.get("type", "").lower()
                    chart_data = viz.get("data", {})
                    
                    if chart_type == "bar":
                        st.markdown("#### ğŸ“Š ë§‰ëŒ€ ì°¨íŠ¸")
                        chart_df = pd.DataFrame(chart_data)
                        fig = px.bar(chart_df, x=chart_df.columns[0], y=chart_df.columns[1])
                        st.plotly_chart(fig)
                    
                    elif chart_type == "pie":
                        st.markdown("#### ğŸ¥§ íŒŒì´ ì°¨íŠ¸")
                        chart_df = pd.DataFrame(chart_data)
                        fig = px.pie(chart_df, values=chart_df.columns[1], names=chart_df.columns[0])
                        st.plotly_chart(fig)
                    
                    elif chart_type == "wordcloud":
                        st.markdown("#### â˜ï¸ ì›Œë“œ í´ë¼ìš°ë“œ")
                        wc = WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(chart_data)
                        plt.figure(figsize=(10, 5))
                        plt.imshow(wc, interpolation="bilinear")
                        plt.axis("off")
                        st.pyplot(plt)
                    
                    else:
                        st.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì‹œê°í™” ìœ í˜•: {chart_type}")

            except json.JSONDecodeError as e:
                st.error(f"ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            except Exception as e:
                st.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                
        else:
            # ê¸°ì¡´ ì±—ë´‡ ëª¨ë“œì¼ ë•Œ
            if 'audio_played' in st.session_state:
                del st.session_state.audio_played
                
            keywords = st.session_state.search_system.extract_keywords(query, progress_bar)
            news_items = st.session_state.search_system.search_with_progressive_keywords(
                keywords, progress_bar
            )
            
            if not news_items:
                try:
                    alt_response = st.session_state.search_system.client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹ í•œì¹´ë“œì˜ CEO ë¬¸ë™ê¶Œ ì‚¬ì¥ì…ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì£¼ì œì— ëŒ€í•´ ì¼ë°˜ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë¬¸ì˜ë¥¼ í•  ê²½ìš°ì— ì‹ í•œì¹´ë“œ ê´€ë ¨ ë¬¸ì˜ë‚˜ ì§ˆë¬¸ì„ í•´ë‹¬ë¼ê³  ë‹µë³€í•˜ê±°ë‚˜ íšŒì‚¬ ê´€ë ¨í•´ì„œ ìì„¸í•˜ê²Œ ë¬¸ì˜í•´ë‹¬ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”."},
                            {"role": "user", "content": query}
                        ],
                        temperature=0.7
                    ).choices[0].message.content

                    st.markdown(f"#### ğŸ’¬ AIë‹µë³€\n{alt_response}")
                    save_message(alt_response, "ai")
                    st.session_state.search_history.append({
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'query': query,
                        'result': f"ğŸ’¬ AIë‹µë³€: {alt_response}"
                    })
                except Exception as e:
                    st.error(f"ëŒ€ì²´ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                return

            result = st.session_state.search_system.analyze_news_content(
                news_items, query, progress_bar
            )
            
            if result:
                st.session_state.current_result = result
                st.markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼")
                
                def extract_section(text, start_marker, end_marker=None):
                    try:
                        if start_marker not in text:
                            return None
                        
                        parts = text.split(start_marker, 1)
                        if len(parts) < 2:
                            return None
                            
                        content = parts[1]
                        
                        if end_marker:
                            if end_marker in content:
                                content = content.split(end_marker)[0]
                        
                        content = re.sub(r'^\s*\[.*?\]\s*', '', content)
                        return content.strip()
                    except Exception:
                        return None
                
                # ê° ì„¹ì…˜ ì¶”ì¶œ
                speech_part = extract_section(result, "[ë¬¸ë™ê¶Œ ì‚¬ì¥ë‹˜ ë§ì”€]", "[ì°¸ê³  ê¸°ì‚¬]")
                ref_part = extract_section(result, "[ì°¸ê³  ê¸°ì‚¬]", "[ì‹ ì…ì‚¬ì› ê°€ì´ë“œ]")
                guide_part = extract_section(result, "[ì‹ ì…ì‚¬ì› ê°€ì´ë“œ]")
                
                # ê²°ê³¼ ì»¨í…Œì´ë„ˆ
                with st.container():
                    if ref_part:
                        st.markdown("#### ğŸ“° ì°¸ê³  ê¸°ì‚¬")
                        lines = ref_part.split('\n')
                        formatted_lines = []
                        for line in lines:
                            line = line.strip()
                            if not line:
                                continue
                            if "URL:" in line:
                                url_parts = line.split("URL:")
                                if len(url_parts) > 1:
                                    url = url_parts[1].strip()
                                    formatted_lines.append(f"- URL: <a href='{url}' target='_blank'>{url}</a>")
                            else:
                                formatted_lines.append(f"- {line}")
                        formatted_ref = '<br>'.join(formatted_lines)
                    
                    if guide_part:
                        st.markdown("#### ğŸ¯ ì‹ ì…ì‚¬ì› ê°€ì´ë“œ")
                        st.markdown(f"""
                        <div style='background-color: #e8f4f9; padding: 20px; border-radius: 10px;'>
                            {guide_part}
                        </div>
                        """, unsafe_allow_html=True)
                    
                    bot_image_path = os.path.join(ASSETS_DIR, 'bot_character.png')
                    
                    if speech_part and os.path.exists(bot_image_path):
                        st.markdown(f"""
                        <div style="display: flex; align-items: center; margin-bottom: 10px;">
                            <img src="data:image/png;base64,{get_image_as_base64(bot_image_path)}" 
                                alt="Bot Icon" 
                                style="width: 30px; height: 30px; margin-right: 10px; border-radius: 50%;">
                            <h3 style="margin: 0; display: inline;">AIë¬¸ë™ê¶Œ ì‚¬ì¥ë‹˜ ë§ì”€</h3>
                        </div>
                        <div style="background-color: #f0f2f6; padding: 20px; border-radius: 10px;">
                            {speech_part}
                        </div>
                        """, unsafe_allow_html=True)
                        
                        st.markdown(f"""
                        <div style="display: flex; align-items: center; margin-bottom: 10px;">
                            <img src="data:image/png;base64,{get_image_as_base64(bot_image_path)}" 
                                alt="Bot Icon" 
                                style="width: 30px; height: 30px; margin-right: 10px; border-radius: 50%;">
                            <h3 style="margin: 0; display: inline;">AIë¬¸ë™ê¶Œ ì‚¬ì¥ë‹˜ ìŒì„±ìœ¼ë¡œ ë“£ê¸°</h3>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        if st.session_state.tts_enabled and 'audio_played' not in st.session_state:
                            st.session_state.search_system.speak_result(result)
                            st.session_state.audio_played = True
                
                # í¬ë§·ëœ ê²°ê³¼ ìƒì„± (íˆìŠ¤í† ë¦¬ìš©)
                formatted_result = f"""### ğŸ“Š ë¶„ì„ ê²°ê³¼

#### ğŸ“° ì°¸ê³  ê¸°ì‚¬
<div style='background-color: #ffffff; padding: 20px; border-radius: 10px; border: 1px solid #e0e0e0;'>
{formatted_ref}
</div>

#### ğŸ¯ ì‹ ì…ì‚¬ì› ê°€ì´ë“œ
<div style='background-color: #e8f4f9; padding: 20px; border-radius: 10px;'>
{guide_part}
</div>

#### ğŸ’¬ ë¬¸ë™ê¶Œ ì‚¬ì¥ë‹˜ ë§ì”€
<div style='background-color: #f0f2f6; padding: 20px; border-radius: 10px;'>
{speech_part}
</div>"""
                
                # ê²€ìƒ‰ ê¸°ë¡ ì €ì¥
                st.session_state.search_history.append({
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'query': query,
                    'result': formatted_result
                })
                save_message(formatted_result, "ai")
            

def initialize_session_state():
    if 'initialized' not in st.session_state:
        st.session_state.tts_enabled = True
        st.session_state.audio_played = False
        st.session_state.messages = []
        st.session_state.search_history = []
        st.session_state.search_system = StreamlitNewsSearchSystem(
            naver_client_id=naver_client_id,
            naver_client_secret=naver_client_secret,
            llm_api_key=llm_api_key,
            xi_api_key=xi_api_key,
            voice_id=voice_id
        )
        st.session_state.initialized = True
# ìºë¦­í„° ì´ë¯¸ì§€ ê²½ë¡œ
user_img = "static/human_character.png"  # ì‚¬ìš©ì ìºë¦­í„° ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
bot_img = "static/bot_character.png"  # ì±—ë´‡ ìºë¦­í„° ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ

if not Path(user_img).exists():
    raise FileNotFoundError(f"File not found: {user_img}")

# ë©”ì‹œì§€ë¥¼ ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
def send_message_with_image(message, role, image_path, save=True):
    """ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ì—¬ ë©”ì‹œì§€ë¥¼ í‘œì‹œ"""
    message_html = f"""
    <div style="display: flex; align-items: flex-start; margin-bottom: 10px;">
        <img src="{image_path}" alt="{role}" style="width: 50px; height: 50px; margin-right: 10px; border-radius: 50%;">
        <div style="background-color: #f1f1f1; padding: 10px; border-radius: 10px; max-width: 80%;">
            {message}
        </div>
    </div>
    """
    st.markdown(message_html, unsafe_allow_html=True)
    if save:
        save_message(message, role)

# ë©”ì‹œì§€ ê¸°ë¡ í‘œì‹œ í•¨ìˆ˜
def paint_history():
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ - ë¶„ì„ ê²°ê³¼ í¬í•¨"""
    if "messages" in st.session_state:
        for message in st.session_state["messages"]:
            if message.get("type") == "analysis":
                # ë¶„ì„ ê²°ê³¼ ë©”ì‹œì§€ í‘œì‹œ
                with st.chat_message("ai", avatar="static/bot_character.png"):
                    display_analysis_results(
                        message["message"]["results"],
                        message["message"]["filename"]
                    )
            else:
                # ì¼ë°˜ ëŒ€í™” ë©”ì‹œì§€ í‘œì‹œ
                send_message(
                    message["message"],
                    message["role"],
                    save=False
                )

def display_bot_section_with_image(title, bot_image_path, content):
    """ì•„ì´ì½˜ ëŒ€ì‹  ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¹ì…˜ì„ í‘œì‹œ"""
    st.markdown(f"""
    <div style="display: flex; align-items: center; margin-bottom: 10px;">
        <img src="{bot_image_path}" alt="Bot Icon" style="width: 30px; height: 30px; margin-right: 10px; border-radius: 50%;">
        <h3 style="margin: 0; display: inline;">{title}</h3>
    </div>
    <div style="background-color: #f0f2f6; padding: 20px; border-radius: 10px;Æ’">
        {content}
    </div>
    """, unsafe_allow_html=True)



# ë©”ì¸ ì½”ë“œ
initialize_session_state()
create_sidebar_with_text_analysis()
paint_history()

# í˜„ì¬ ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ UI ë Œë”ë§
if st.session_state.analysis_mode:
    # í…ìŠ¤íŠ¸ ë¶„ì„ ì±—ë´‡ ëª¨ë“œì¼ ë•Œë§Œ íŒŒì¼ ì—…ë¡œë“œ í‘œì‹œ
    st.markdown("---")
    st.markdown("### ğŸ“‚ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ë¶„ì„í•  í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (CSV ë˜ëŠ” XLSX)", type=["csv", "xlsx"])

    if uploaded_file:
        file_analysis_result = analyze_uploaded_file(uploaded_file)
        if file_analysis_result:  # ìœ íš¨í•œ íŒŒì¼ë§Œ ì²˜ë¦¬
            text_data, df = file_analysis_result
            st.success("íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ë¶„ì„ ë²„íŠ¼ ì¶”ê°€
            if st.button("ë¶„ì„ ì‹œì‘"):
                with st.spinner("LLMì„ í†µí•´ ë¶„ì„ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                    # LLMì— í…ìŠ¤íŠ¸ ë°ì´í„° ì „ë‹¬
                    llm_prompt = f"""
                    ì•„ë˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ **JSONìœ¼ë¡œë§Œ** ë°˜í™˜í•˜ì„¸ìš”. 
                    JSON ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ë°˜í™˜ ê°’ì€ ë¬¸ìì—´ì´ ì•„ë‹ˆë©° JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

                    ë°ì´í„°:
                    {text_data[:1000]} 
                    
                    ë¶„ì„ ìš”êµ¬ì‚¬í•­:
                    1. í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ ë¶„ì„: "keyword_frequency"ì— JSON ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.
                    2. ì£¼ì œ ë¶„í¬: "topic_distribution"ì— JSON ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.
                    3. ê°ì„± ë¶„ì„: "sentiment_analysis"ì— JSON ê°ì²´ë¡œ ê¸ì •, ë¶€ì •, ì¤‘ë¦½ ë¹„ìœ¨ì„ ë°˜í™˜í•˜ì„¸ìš”.
                    4. ì£¼ìš” ì¸ì‚¬ì´íŠ¸: "key_insights"ì— JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

                    JSON í˜•ì‹ ì˜ˆ:
                    {{
                        "keyword_frequency": [
                            {{"keyword": "example", "count": 10}},
                            ...
                        ],
                        "topic_distribution": [
                            {{"topic": "example_topic", "percentage": 50}},
                            ...
                        ],
                        "sentiment_analysis": {{
                            "positive_score": 60,
                            "negative_score": 30,
                            "neutral_score": 10
                        }},
                        "key_insights": [
                            "Insight 1",
                            "Insight 2"
                        ]
                    }}
"""
                    try:
                        response = st.session_state.search_system.client.chat.completions.create(
                            model="gpt-4o-mini",
                            messages=[
                                {"role": "system", "content": "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                                {"role": "user", "content": llm_prompt}
                            ],
                            temperature=0.2
                        )
                        
                        # LLM ì‘ë‹µ ë‚´ìš© ë¡œê·¸
                        if response:
                            st.write("LLM ì‘ë‹µ ë‚´ìš©:", response.choices[0].message.content)
                        else:
                            st.error("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")

                        # LLM ì‘ë‹µ ì²˜ë¦¬
                        raw_response = response.choices[0].message.content
                        analysis_results = json.loads(raw_response)

                        # ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
                        st.markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼")
                        # ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                        display_analysis_results(analysis_results, uploaded_file.name)
                        
                        # ë¶„ì„ ê²°ê³¼ë¥¼ ëŒ€í™” ì´ë ¥ì— ì €ì¥
                        save_analysis_to_history(
                            st_state=st.session_state,
                            analysis_results=analysis_results,
                            uploaded_filename=uploaded_file.name
                        )
                    
                        # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€
                        st.success("ë¶„ì„ ê²°ê³¼ê°€ ëŒ€í™” ì´ë ¥ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    except json.JSONDecodeError as e:
                        st.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        st.write("LLM ì‘ë‹µ ë‚´ìš©:", response.choices[0].message.content)
                    except Exception as e:
                        st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

                        # ê²°ê³¼ ì‹œê°í™”
                        st.markdown("### ğŸ“Š í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼")

                        # í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜
                        st.markdown("#### ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„")
                        keyword_df = pd.DataFrame(analysis_results['keyword_frequency'])
                        keyword_chart = px.bar(keyword_df, x="keyword", y="count", title="í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜")
                        st.plotly_chart(keyword_chart, use_container_width=True)

                        # ì£¼ì œ ë¶„í¬
                        st.markdown("#### ì£¼ì œ ë¶„í¬")
                        topic_df = pd.DataFrame(analysis_results['topic_distribution'])
                        topic_chart = px.pie(topic_df, values="percentage", names="topic", title="ì£¼ì œ ë¶„í¬")
                        st.plotly_chart(topic_chart, use_container_width=True)

                        # ê°ì„± ë¶„ì„
                        st.markdown("#### ê°ì„± ë¶„ì„")
                        sentiment = analysis_results['sentiment_analysis']
                        total_score = sum(sentiment.values())
                        positive_ratio = (sentiment["positive_score"] / total_score) * 100

                        sentiment_chart = go.Figure(go.Indicator(
                            mode="gauge+number",
                            value=positive_ratio,
                            title={'text': "ê¸ì •ë„ ë¹„ìœ¨"},
                            gauge={
                                'axis': {'range': [0, 100]},
                                'steps': [
                                    {'range': [0, 50], 'color': "lightgray"},
                                    {'range': [50, 100], 'color': "lightblue"}
                                ]
                            }
                        ))
                        st.plotly_chart(sentiment_chart, use_container_width=True)

                        # ì£¼ìš” ì¸ì‚¬ì´íŠ¸
                        st.markdown("#### ì£¼ìš” ì¸ì‚¬ì´íŠ¸")
                        for insight in analysis_results['key_insights']:
                            st.markdown(f"- {insight}")

                    except Exception as e:  # ì´ ì¤„ì˜ ë“¤ì—¬ì“°ê¸°ê°€ ë§ì§€ ì•Šì•„ ì˜¤ë¥˜ ë°œìƒ
                        st.error(f"LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        else:
            st.error("íŒŒì¼ì„ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
else:                            
    # ê¸°ë³¸ ì±—ë´‡ ëª¨ë“œì¼ ë•Œ
    query = st.chat_input("ê¶ê¸ˆí•œ ì‚¬í•­ì„ ììœ ë¡­ê²Œ ë¬¼ì–´ë³´ì„¸ìš”")
    if query:
        send_message(query, "human")
        progress_bar = st.progress(0)
        with st.chat_message("ai", avatar="static/bot_character.png"):
            main(query)

        
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í›„ì— ì‚¬ì´ë“œë°” ì¶”ê°€
with st.sidebar:
    # ì‚¬ìš© ê°€ì´ë“œ
    st.markdown("### ğŸ¯ ì‚¬ìš© ê°€ì´ë“œ")
    st.markdown("""
    1. íšŒì‚¬ì— ë¬¸ì˜í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”
    2. ê²€ìƒ‰ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
    3. ë¶„ì„ ê²°ê³¼ê°€ í‘œì‹œë˜ë©° ìŒì„±ìœ¼ë¡œë„ ë“¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """)
    
    st.markdown("---")
    
    # ê²€ìƒ‰ ê¸°ë¡
    if st.session_state.search_history:
        st.markdown(f"ì´ ê²€ìƒ‰ íšŸìˆ˜: {len(st.session_state.search_history)}íšŒ")